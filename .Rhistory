filter(prob_automation_class == "2) Medium") %>%
group_by(STATE) %>%
summarise(current_jobs = sum(TOT_EMP)/10**6),
locationvar = "STATE",
colorvar = "current_jobs",
options = list(
region = "US",
displayMode = "regions",
resolution = "provinces",
colorAxis="{colors:['white', 'orange']}",
width = "700",
height = "350"
)
)
renderGvis({
gvisGeoChart(df_jobs = data_job_filter() %>%
filter(prob_automation_class == "2) Medium") %>%
group_by(STATE) %>%
summarise(current_jobs = sum(TOT_EMP)/10**6),
locationvar = "STATE",
colorvar = "current_jobs",
options = list(
region = "US",
displayMode = "regions",
resolution = "provinces",
colorAxis="{colors:['white', 'orange']}",
width = "700",
height = "350"
)
)
})
gvisGeoChart(df_jobs = data_job_filter() %>%
filter(prob_automation_class == "2) Medium") %>%
group_by(STATE) %>%
summarise(current_jobs = sum(TOT_EMP)/10**6),
locationvar = "STATE",
colorvar = "current_jobs",
options = list(
region = "US",
displayMode = "regions",
resolution = "provinces",
colorAxis="{colors:['white', 'orange']}",
width = "700",
height = "350"
)
)
gvisGeoChart(data = df_jobs %>%
filter(prob_automation_class == "2) Medium") %>%
group_by(STATE) %>%
summarise(current_jobs = sum(TOT_EMP)/10**6),
locationvar = "STATE",
colorvar = "current_jobs",
options = list(
region = "US",
displayMode = "regions",
resolution = "provinces",
colorAxis="{colors:['white', 'orange']}",
width = "700",
height = "350"
)
)
data = df_jobs %>%
filter(prob_automation_class == "2) Medium") %>%
group_by(STATE) %>%
summarise(current_jobs = sum(TOT_EMP)/10**6)
View(data)
data = df_jobs %>%
group_by(STATE) %>%
summarise(n_state=n()) %>%
filter(prob_automation_class == "1) High") %>%
summarise(n_cat_state=n()) %>%
mutate(freq = n_cat_state/n_state)
data = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
summarise(n_state=n())
View(data)
data = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
summarise(n_state_prob=n()) %>%
group_by(STATE) %>%
summarise(n_state=n()) %>%
%>%
filter(prob_automation_class == "1) High") %>%
summarise(n_cat_state=n()) %>%
mutate(freq = n_cat_state/n_state)
data = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
summarise(n_state_prob=n()) %>%
group_by(STATE) %>%
summarise(n_state=n()) %>%
%>%
filter(prob_automation_class == "1) High") %>%
summarise(n_cat_state=n()) %>%
mutate(freq = n_cat_state/n_state)
data = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
summarise(n_state_prob=n()) %>%
group_by(STATE) %>%
summarise(n_state=n())
View(data)
data = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
summarise(n_state_prob=n()) %>%
group_by(STATE) %>%
summarise(n_state=sum(n_state_prob))
View(data)
data = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
mutate(n_state_prob=n()) %>%
group_by(STATE) %>%
summarise(n_state=sum(n_state_prob))
View(data)
data = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
mutate(n_state_prob=n()) %>%
group_by(STATE) %>%
mutate(n_state=sum(n_state_prob))
View(data)
data = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
mutate(n_state_prob=n()) %>%
group_by(STATE) %>%
summarise(n_state=sum(n_state_prob))
View(data)
job_states = df_jobs %>%
group_by(STATE) %>%
mutate(n_state=n())
View(job_states)
job_states = df_jobs %>%
group_by(STATE) %>%
summarise(n_state=n())
df_job_prob_states = left_join(job_prob_states,job_states,by="STATE")
job_states = df_jobs %>%
group_by(STATE) %>%
summarise(n_state=n())
job_prob_states = df_jobs %>%
group_by(prob_automation_class,STATE) %>%
summarise(n_state_prob=n())
df_job_prob_states = left_join(job_prob_states,job_states,by="STATE")
View(df_job_prob_states)
df_job_prob_states = left_join(job_prob_states,job_states,by="STATE") %>%
mutate(proportion = n_state_prob/n_state)
View(df_job_prob_states)
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
df_jobs %>% ggplot() + geom_point(aes(x=Probability,y=H_MEAN))
View(df_jobs)
View(df_jobs)
df_jobs %>% ggplot() + geom_point(aes(x=Probability,y=A_MEAN))
df_jobs %>% ggplot() + geom_point(aes(x=Probability,y=A_MEAN))
View(data_df)
df_jobs %>% group_by(OCC_CODE) %>%
summarise(job_mean = mean(A_MEAN)) %>%
ggplot() + geom_point(aes(x=Probability,y=A_MEAN))
df_jobs %>% group_by(OCC_CODE) %>%
summarise(job_mean = mean(A_MEAN)) %>%
ggplot() + geom_point(aes(x=Probability,y=A_MEAN))
View(df_jobs)
df_jobs %>% group_by(OCC_CODE, rounded_prob) %>%
summarise(job_mean = mean(A_MEAN)) %>%
ggplot() + geom_point(aes(x=Probability,y=A_MEAN))
df_jobs %>% group_by(OCC_CODE, rounded_prob) %>%
summarise(job_mean = mean(A_MEAN)) %>%
ggplot() + geom_point(aes(x=rounded_prob,y=A_MEAN))
group_by(OCC_CODE) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP))
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP )
sal = df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP )
View(sal)
group_by(OCC_CODE) %>%
mutate(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
summarise(salary_mean)
sal = df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP )
sal = df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE) %>%
mutate(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
summarise(salary_mean)
sal = df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE) %>%
mutate(salary_mean = sum(salary_weighted)/sum(TOT_EMP))
sal = df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP))
View(sal)
sal = df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot() + geom_point()
sal = df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot() + geom_point(aes(x=Probability,y=salary_mean))
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot() + geom_point(aes(x=Probability,y=salary_mean))
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot() + geom_point(aes(x=Probability,y=salary_mean))
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot() + geom_point(aes(x=Probability,y=salary_mean)) + geom_line()
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot() + geom_point(aes(x=Probability,y=salary_mean)) + geom_smooth(method = 'lm')
ggplot(aes(x=Probability,y=salary_mean) + geom_point() + geom_smooth(method = 'lm')
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot(aes(x=Probability,y=salary_mean)) + geom_point() + geom_smooth(method = 'lm')
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise(salary_mean = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot(aes(x=Probability,y=salary_mean)) + geom_point() + geom_smooth(method = 'lm')
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
summarise(Mean salary k = sum(salary_weighted)/(10**3*sum(TOT_EMP)) %>%
ggplot(aes(x=Probability,y=Mean_salary_k)) + geom_point() + geom_smooth(method = 'lm')
summarise(Mean_salary_k = sum(salary_weighted)/(10^3*sum(TOT_EMP)) %>%
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise(Salary = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot(aes(x=Probability,y=Mean_salary_k)) + geom_point() + geom_smooth(method = 'lm')
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise( = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot(aes(x=Probability,y=Mean_salary_k)) + geom_point() + geom_smooth(method = 'lm')
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise(Mean_salary_k = sum(salary_weighted)/sum(TOT_EMP)) %>%
ggplot(aes(x=Probability,y=Mean_salary_k)) + geom_point() + geom_smooth(method = 'lm')
df_jobs %>% mutate(salary_weighted = A_MEAN * TOT_EMP ) %>%
group_by(OCC_CODE, Probability) %>%
summarise(Mean_salary_k = sum(salary_weighted)/sum(TOT_EMP)/10**3) %>%
ggplot(aes(x=Probability,y=Mean_salary_k)) + geom_point() + geom_smooth(method = 'lm')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
#install.packages("shinyWidgets")
#install.packages('rsconnect')
library(rsconnect)
library(tidyverse)
# READ DATA ####
#Read data, fix to be relative references
oxford_pred = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/OxfordPred.csv",stringsAsFactors=FALSE)
jobs_data_2010 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/jobs_data_2010.csv",stringsAsFactors=FALSE)
jobs_data_2012 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/jobs_data_2012.csv",stringsAsFactors=FALSE)
jobs_data_2014 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/jobs_data_2014.csv",stringsAsFactors=FALSE)
jobs_data_2016 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/jobs_data_2016.csv",stringsAsFactors=FALSE)
jobs_data_2018 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/jobs_data_2018.csv",stringsAsFactors=FALSE)
jobs_map = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/job_mapping.csv",stringsAsFactors=FALSE)
# Read abilities data 2010
job_abilities_2010 = read.delim("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2010/Abilities.txt",stringsAsFactors=FALSE)
job_skills_2010 = read.delim("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2010/Skills.txt",stringsAsFactors=FALSE)
job_knowledge_2010 = read.delim("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2010/Knowledge.txt",stringsAsFactors=FALSE)
job_activities_2010 = read.delim("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2010/Work Activities.txt",stringsAsFactors=FALSE)
job_context_2010 = read.delim("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2010/Work Context.txt",stringsAsFactors=FALSE)
# Read abilities data 2018
job_abilities_2018 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2018/Abilities.csv",stringsAsFactors=FALSE)
job_skills_2018 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2018/Skills.csv",stringsAsFactors=FALSE)
job_knowledge_2018 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2018/Knowledge.csv",stringsAsFactors=FALSE)
job_activities_2018 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2018/Work Activities.csv",stringsAsFactors=FALSE)
job_context_2018 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/2018/Work Context.csv",stringsAsFactors=FALSE)
shared_columns = c('O.NET.SOC.Code','Scale.ID','Element.Name','Element.Name','Data.Value')
# DATA PREP - ONET tables 2010 ####
# Define list of bottleneck variables
shared_columns = c('O.NET.SOC.Code','Scale.ID','Element.Name','Data.Value')
job_abilities_2010 = job_abilities_2010 %>% select(shared_columns)
job_skills_2010 = job_skills_2010 %>% select(shared_columns)
job_knowledge_2010 = job_knowledge_2010 %>% select(shared_columns)
job_activities_2010 = job_activities_2010 %>% select(shared_columns)
job_context_2010 = job_context_2010 %>% select(shared_columns)
bottleneck_features = c('Finger Dexterity','Manual Dexterity','Originality','Fine Arts',
'Social Perceptiveness','Negotiation','Persuasion',
'Assisting and Caring for Others','Cramped Work Space, Awkward Positions')
# Rearrange abilities tables to allow aggregation
capabilities_2010 = rbind(job_abilities_2010,job_skills_2010,job_knowledge_2010,job_activities_2010,job_context_2010)
capabilities_2010_cond = capabilities_2010 %>%
filter(Scale.ID %in% c('LV','CX')) %>%
mutate(
OCC_CODE = substr(O.NET.SOC.Code,1,7),
top_level_abilities = case_when(
Element.Name %in% c('Arm-Hand Steadiness','Control Precision','Finger Dexterity','Manual Dexterity',
'Multilimb Coordination','Rate Control','Reaction Time','Response Orientation',
'Speed of Limb Movement','Wrist-Finger Speed') ~ 'psychomotor_abilities',
Element.Name %in% c('Dynamic Flexibility','Dynamic Strength','Explosive Strength','Extent Flexibility',
'Gross Body Coordination','Gross Body Equilibrium','Stamina','Static Strength',
'Trunk Strength') ~ 'physical_abilities',
Element.Name %in% c('Auditory Attention','Depth Perception','Far Vision','Glare Sensitivity',
'Hearing Sensitivity','Near Vision','Night Vision','Peripheral Vision',
'Sound Localization','Speech Clarity','Speech Recognition','Visual Color Discrimination') ~ 'sensory_abilities',
TRUE ~ 'cognitive_abilities'),
bottleneck_features = case_when(
Element.Name %in% bottleneck_features ~ 'Bottleneck',
TRUE ~ 'Not bottleneck')
) %>%
filter(top_level_abilities != '' | bottleneck_features == 'Bottleneck') %>%
select('OCC_CODE','Scale.ID','Element.Name','Element.Name','top_level_abilities','bottleneck_features','Data.Value')
capabilities_2010_mod = capabilities_2010_cond %>%
group_by(OCC_CODE, top_level_abilities) %>%
summarise(mean_value=mean(Data.Value)) %>%
spread(top_level_abilities,mean_value)
capabilities_2010_mod_2 = capabilities_2010_cond %>%
filter(bottleneck_features == 'Bottleneck') %>%
group_by(OCC_CODE,Element.Name) %>%
summarise(mean_value=mean(Data.Value)) %>%
spread(Element.Name,mean_value) %>%
rename(finger_dexterity = 'Finger Dexterity',manual_dexterity='Manual Dexterity',fine_arts='Fine Arts',social_perceptiveness = 'Social Perceptiveness',
assisting_and_caring = 'Assisting and Caring for Others',cramped_work_space = 'Cramped Work Space, Awkward Positions')
# Join skills
capabilities_final = left_join(capabilities_2010_mod,capabilities_2010_mod_2,by = "OCC_CODE")
# Define averages within six digit job code
# Jobs counts
jobs_data_2010 = jobs_data_2010 %>% mutate(YEAR = '2010')
jobs_data_2012 = jobs_data_2010 %>% mutate(YEAR = '2012')
jobs_data_2014 = jobs_data_2014 %>% mutate(YEAR = '2014')
jobs_data_2016 = jobs_data_2014 %>% mutate(YEAR = '2016')
jobs_data_2018 = jobs_data_2018 %>% mutate(YEAR = '2018')
# Align column names
names(jobs_data_2010)[names(jobs_data_2010) == "GROUP"] = "OCC_GROUP"
names(jobs_data_2010)[names(jobs_data_2010) == "LOC.QUOTIENT"] = "LOC_Q"
names(jobs_data_2012)[names(jobs_data_2012) == "GROUP"] = "OCC_GROUP"
names(jobs_data_2012)[names(jobs_data_2012) == "LOC.QUOTIENT"] = "LOC_Q"
0.57*128/(138+4)
#Merge clean BLS datasets
jobs_data = rbind(jobs_data_2010,jobs_data_2012,jobs_data_2014,jobs_data_2016,jobs_data_2018)
#Apply formats
jobs_data[,7:26] = lapply(jobs_data[,7:26], function(x) as.numeric(gsub(",","",x)))
str(jobs_data)
total_jobs = jobs_data %>% filter(TOT_EMP >=0) %>%
group_by(YEAR,OCC_GROUP) %>%
summarise(a= sum(TOT_EMP))
jobs_data = jobs_data[jobs_data$OCC_GROUP != 'total',]
jobs_data = jobs_data[jobs_data$OCC_GROUP != 'major',]
# DATA PREP - Oxford paper ####
#Fix aggregation of job groups
test_gen = oxford_pred %>% filter(grepl('Other',oxford_pred$Occupation))
test_gen2 = jobs_data %>% filter(grepl('Other',jobs_data$OCC_TITLE))
oxford_pred = oxford_pred %>% mutate(OCC_CODE = substr(SOC_Code_Clean,2,8))
df_jobs$TOT_EMP[is.na(df_jobs$TOT_EMP)] =0
#Fix aggregation of job groups before merge
jobs_data = jobs_data %>% mutate(OCC_CODE = case_when(
substr(OCC_CODE,1,2) == '25' & (as.numeric(substr(OCC_CODE,4,7)) %in% 1000:1199) ~ '25-1000',
substr(OCC_CODE,1,2) == '29' & (as.numeric(substr(OCC_CODE,4,7)) %in% 1060:1069) ~ '29-1060',
substr(OCC_CODE,1,2) == '15' & (as.numeric(substr(OCC_CODE,4,7)) %in% 1150:1159) ~ '15-1150',
substr(OCC_CODE,1,2) == '45' & (as.numeric(substr(OCC_CODE,4,7)) %in% 2090:2099) ~ '45-2090',
TRUE ~ OCC_CODE)
)
# Merge data on SOC_Code, add fields
df_jobs = left_join(jobs_data,oxford_pred,by = "OCC_CODE")
df_jobs = df_jobs %>% mutate(major_job_group = paste0(substr(OCC_CODE,0,2),"-0000"))
df_jobs = left_join(df_jobs,jobs_map,by = "major_job_group")
df_jobs = left_join(df_jobs,capabilities_final,by = "OCC_CODE")
df_jobs = df_jobs %>% mutate(prob_automation_class = case_when(
Probability<=0.3 ~ '3) Low',
Probability>0.3 & Probability<=0.6  ~ '2) Medium',
Probability>0.6 ~ '1) High',
is.na(Probability) ~ '4) New / Unclassified'
),
econ_value = TOT_EMP* H_MEAN
)
df_ML = df_jobs %>%
group_by(OCC_CODE, OCC_TITLE, fine_arts, finger_dexterity, manual_dexterity, social_perceptiveness, Negotiation, Originality, Persuasion,assisting_and_caring,cramped_work_space) %>%
summarise(label_ML = max(Label),probability_ML = max(Probability)) %>%
filter(!is.na(fine_arts))
df_ML_train = df_ML %>% filter(!is.na(label_ML))
df_ML_train$label_ML = factor(df_ML_train$label_ML)
# Simple logistic regression model
mylogit = glm(label_ML ~ fine_arts + finger_dexterity + manual_dexterity + social_perceptiveness, data = df_ML_train, family = "binomial")
summary(mylogit)
df_ML_train$prob_est = predict(mylogit, newdata= df_ML_train, type = "response")
df_ML_train %>%
ggplot(aes(x=probability_ML,y=prob_est)) + geom_point() + geom_smooth(method="lm")
df_ML_train$label_ML
write.csv(df_ML,"~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny/python_ML/ML_data_2010.csv", row.names = FALSE)
# DATA PREP - ONET tables 2018 ####
# Define list of bottleneck variables
shared_columns = c('O.NET.SOC.Code','Scale.ID','Element.Name','Data.Value')
job_abilities_2018 = job_abilities_2018 %>% select(shared_columns)
job_skills_2018 = job_skills_2018 %>% select(shared_columns)
job_knowledge_2018 = job_knowledge_2018 %>% select(shared_columns)
job_activities_2018 = job_activities_2018 %>% select(shared_columns)
job_context_2018 = job_context_2018 %>% select(shared_columns)
bottleneck_features = c('Finger Dexterity','Manual Dexterity','Originality','Fine Arts',
'Social Perceptiveness','Negotiation','Persuasion',
'Assisting and Caring for Others','Cramped Work Space, Awkward Positions')
# Rearrange abilities tables to allow aggregation
capabilities_2018 = rbind(job_abilities_2018,job_skills_2018,job_knowledge_2018,job_activities_2018,job_context_2018)
capabilities_2018_cond = capabilities_2018 %>%
filter(Scale.ID %in% c('LV','CX')) %>%
mutate(
OCC_CODE = substr(O.NET.SOC.Code,1,7),
top_level_abilities = case_when(
Element.Name %in% c('Arm-Hand Steadiness','Control Precision','Finger Dexterity','Manual Dexterity',
'Multilimb Coordination','Rate Control','Reaction Time','Response Orientation',
'Speed of Limb Movement','Wrist-Finger Speed') ~ 'psychomotor_abilities',
Element.Name %in% c('Dynamic Flexibility','Dynamic Strength','Explosive Strength','Extent Flexibility',
'Gross Body Coordination','Gross Body Equilibrium','Stamina','Static Strength',
'Trunk Strength') ~ 'physical_abilities',
Element.Name %in% c('Auditory Attention','Depth Perception','Far Vision','Glare Sensitivity',
'Hearing Sensitivity','Near Vision','Night Vision','Peripheral Vision',
'Sound Localization','Speech Clarity','Speech Recognition','Visual Color Discrimination') ~ 'sensory_abilities',
TRUE ~ 'cognitive_abilities'),
bottleneck_features = case_when(
Element.Name %in% bottleneck_features ~ 'Bottleneck',
TRUE ~ 'Not bottleneck')
) %>%
filter(top_level_abilities != '' | bottleneck_features == 'Bottleneck') %>%
select('OCC_CODE','Scale.ID','Element.Name','Element.Name','top_level_abilities','bottleneck_features','Data.Value')
capabilities_2018_mod = capabilities_2018_cond %>%
group_by(OCC_CODE, top_level_abilities) %>%
summarise(mean_value=mean(Data.Value)) %>%
spread(top_level_abilities,mean_value)
capabilities_2018_mod_2 = capabilities_2018_cond %>%
filter(bottleneck_features == 'Bottleneck') %>%
group_by(OCC_CODE,Element.Name) %>%
summarise(mean_value=mean(Data.Value)) %>%
spread(Element.Name,mean_value) %>%
rename(finger_dexterity = 'Finger Dexterity',manual_dexterity='Manual Dexterity',fine_arts='Fine Arts',social_perceptiveness = 'Social Perceptiveness',
assisting_and_caring = 'Assisting and Caring for Others',cramped_work_space = 'Cramped Work Space, Awkward Positions')
# Join skills
capabilities_final_2018 = left_join(capabilities_2018_mod,capabilities_2018_mod_2,by = "OCC_CODE")
write.csv(capabilities_final_2018,"~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny/python_ML/ML_data_2018.csv", row.names = FALSE)
# Add on new ML predictions
ML_pred_2018 = read.csv("~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny/python_ML/ML_output_2018.csv",stringsAsFactors=FALSE)
join_ML_pred_2018 = ML_pred_2018 %>% select('OCC_CODE','prob_class_new')
df_jobs = left_join(df_jobs,join_ML_pred_2018,by = "OCC_CODE")
# GAP FILLING ####
df_jobs$TOT_EMP[is.na(df_jobs$TOT_EMP)] =0
df_jobs$econ_value[is.na(df_jobs$econ_value)] =0
df_jobs$A_MEAN[is.na(df_jobs$A_MEAN)] =0
df_jobs$cognitive_abilities[is.na(df_jobs$cognitive_abilities)] =0
df_jobs$physical_abilities[is.na(df_jobs$physical_abilities)] =0
df_jobs$psychomotor_abilities[is.na(df_jobs$psychomotor_abilities)] =0
df_jobs$sensory_abilities[is.na(df_jobs$sensory_abilities)] =0
df_jobs$finger_dexterity[is.na(df_jobs$finger_dexterity)] =0
df_jobs$manual_dexterity[is.na(df_jobs$manual_dexterity)] =0
df_jobs$Negotiation[is.na(df_jobs$Negotiation)] =0
df_jobs$Originality[is.na(df_jobs$Originality)] =0
df_jobs$Persuasion[is.na(df_jobs$Persuasion)] =0
df_jobs$fine_arts[is.na(df_jobs$fine_arts)] =0
df_jobs$social_perceptiveness[is.na(df_jobs$social_perceptiveness)] =0
df_jobs$assisting_and_caring[is.na(df_jobs$assisting_and_caring)] =0
df_jobs$cramped_work_space[is.na(df_jobs$cramped_work_space)] =0
df_jobs$prob_class_new[is.na(df_jobs$prob_class_new)] = "4) New / Unclassified"
#HACK
df_jobs = df_jobs %>% mutate(rounded_prob = round(Probability,digits=2))
#REMOVING FAR TOO MANY...
df_jobs = df_jobs[df_jobs$TOT_EMP > 0,]
#REMOVES TOTALS BY AREA
#df_jobs = df_jobs[df_jobs$OCC_CODE != '00-0000',]
#TURN ON/ OFF
#df_jobs = df_jobs[df_jobs$OCC_GROUP== 'detailed',]
# WHY SO MANY NAs??
#VIEW GApS
test = df_jobs[is.na(df_jobs$Probability),,]
test2 = test %>% group_by(OCC_TITLE,OCC_CODE) %>%
summarise(count_n = n())
test3 = oxford_pred %>% mutate(last = substr(SOC_Code_Clean,8,8) )
test3 = test3[test3$last =='0',]
# WRITE CLEAN DATASET TO DRIVE ####
#Filter to necessary columns
keep_columns = c(
'YEAR',
'STATE',
'OCC_CODE',
'OCC_TITLE',
'TOT_EMP',
'A_MEAN',
'Probability',
'rounded_prob',
'major_job_description',
'top_level_job_category_desc',
'Label',
'prob_automation_class',
'cognitive_abilities',
'physical_abilities',
'psychomotor_abilities',
'sensory_abilities',
'fine_arts',
'finger_dexterity',
'manual_dexterity',
'social_perceptiveness',
'Negotiation',
'Originality',
'Persuasion',
'assisting_and_caring',
'cramped_work_space',
'prob_class_new'
)
df_jobs = df_jobs %>%
select(keep_columns)
write.csv(df_jobs,"~/Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/clean_dataset_comb.csv", row.names = FALSE)
View(df_jobs)
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp()
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
runApp('Analytics/Courses/NYC_DSA/R_studio/Projects/ML_impacts/Shiny')
